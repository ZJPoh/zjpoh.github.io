---
title: "Readings"
date: 2023-05-09
permalink: /posts/2023/05/readings
excerpt: "This is where I keep my reading lists"
tags:
  - ml
  - papers
---

# Forecasting
* Lim, Bryan, et al. "Temporal fusion transformers for interpretable multi-horizon time series forecasting." International Journal of Forecasting 37.4 (2021): 1748-1764. <https://arxiv.org/abs/1912.09363>
* Zhou, Tian, et al. "Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting." International Conference on Machine Learning. PMLR, 2022. <https://arxiv.org/abs/2201.12740>
* Nie, Yuqi, et al. "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers." arXiv preprint arXiv:2211.14730 (2022). <https://arxiv.org/abs/2211.14730>
* Das, Abhimanyu, et al. "Long-term Forecasting with TiDE: Time-series Dense Encoder." arXiv preprint arXiv:2304.08424 (2023). <https://arxiv.org/abs/2304.08424>
*

# Generic ML
* Shazeer, Noam, et al. "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer." arXiv preprint arXiv:1701.06538 (2017). <https://arxiv.org/abs/1701.06538>
* Zhou, Yanqi, et al. "Mixture-of-experts with expert choice routing." Advances in Neural Information Processing Systems 35 (2022): 7103-7114. <https://arxiv.org/abs/2202.09368>

* Balestriero, Randall, et al. "A Cookbook of Self-Supervised Learning." arXiv preprint arXiv:2304.12210 (2023). <https://arxiv.org/abs/2304.12210>

